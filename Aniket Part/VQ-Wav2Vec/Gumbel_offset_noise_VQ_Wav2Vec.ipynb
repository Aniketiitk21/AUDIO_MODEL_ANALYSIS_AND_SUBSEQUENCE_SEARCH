{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA4G5oJvD6HT",
        "outputId": "6739114e-06ee-42a8-802e-ba4776048133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrubberband\n",
            "  Downloading pyrubberband-0.3.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyrubberband) (1.16.0)\n",
            "Collecting pysoundfile>=0.8.0 (from pyrubberband)\n",
            "  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from pysoundfile>=0.8.0->pyrubberband) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->pysoundfile>=0.8.0->pyrubberband) (2.22)\n",
            "Building wheels for collected packages: pyrubberband\n",
            "  Building wheel for pyrubberband (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrubberband: filename=pyrubberband-0.3.0-py3-none-any.whl size=4264 sha256=89db85b57b486c46f1bf198b78af4e395e54e9314a5d38ca0ca82c97e8e8e84c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/2d/f0/bb68fbfe67a42c858a79412321d28589218cbfe114c48ce664\n",
            "Successfully built pyrubberband\n",
            "Installing collected packages: pysoundfile, pyrubberband\n",
            "Successfully installed pyrubberband-0.3.0 pysoundfile-0.9.0.post1\n",
            "Collecting torch-time-stretch\n",
            "  Downloading torch_time_stretch-1.0.3-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torch-time-stretch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torch-time-stretch) (2.2.1+cu121)\n",
            "Collecting primePy>=1.3 (from torch-time-stretch)\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torch-time-stretch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torch-time-stretch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torch-time-stretch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torch-time-stretch) (1.3.0)\n",
            "Installing collected packages: primePy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch-time-stretch\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 primePy-1.3 torch-time-stretch-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyrubberband\n",
        "!pip install torch-time-stretch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkxucFWiCV-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy import signal\n",
        "import pyrubberband as pyrb\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch_time_stretch import time_stretch\n",
        "import numpy as np\n",
        "# import shutil\n",
        "import tempfile\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "\n",
        "def read_audio(filepath, fs=16000,  mono=True, normalize=False, preemphasis=False):\n",
        "    \"\"\"\n",
        "    Reads audio file stored at <filepath>\n",
        "    Parameters:\n",
        "        filepath (str): audio file path\n",
        "        fs (int, optional): samping rate\n",
        "        mono (boolean, optional): return single channel\n",
        "        normalize(boolean, optional): peak normalization of signal\n",
        "        preemphasis (boolean, optional): apply pre-emphasis filter\n",
        "    Returns:\n",
        "        waveform (tensor): audio signal, dim(N,)\n",
        "    \"\"\"\n",
        "    assert isinstance(filepath, str), \"filepath must be specified as string\"\n",
        "    assert os.path.exists(filepath), f\"{filepath} does not exist.\"\n",
        "\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(filepath)\n",
        "        # mono channel\n",
        "        if waveform.shape[0] == 2 and mono is True: waveform = waveform[0]\n",
        "        else: waveform = waveform.reshape(-1)\n",
        "        # preemphasis\n",
        "        if preemphasis:\n",
        "            waveform = pre_emphasis(waveform)\n",
        "        # resample\n",
        "        if sr != fs:\n",
        "            resampler = T.Resample(sr, fs, dtype=waveform.dtype)\n",
        "            waveform = resampler(waveform)\n",
        "        # normalize\n",
        "        if normalize:\n",
        "            waveform = rms_normalize(waveform)\n",
        "        return waveform\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def peak_normalize(waveform):\n",
        "    \"\"\"\n",
        "    Peak normalizes the <waveform>\n",
        "    Parameter:\n",
        "        waveform (tensor): waveform, dims: (N,)\n",
        "    \"\"\"\n",
        "    return waveform/torch.max(torch.abs(waveform))\n",
        "\n",
        "\n",
        "def rms_normalize(waveform, r=-10):\n",
        "    \"\"\"\n",
        "    RMS-normalization of  <waveform>\n",
        "    Parameter:\n",
        "        waveform (tensor): waveform, dims: (N,)\n",
        "        rms (float): rms in dB\n",
        "    \"\"\"\n",
        "    current_rms = torch.pow(torch.mean(torch.pow(waveform,2)) ,0.5)\n",
        "    scaling_factor = (10**(r/10))/current_rms\n",
        "    return waveform*scaling_factor\n",
        "\n",
        "\n",
        "def pre_emphasis(waveform, coeff=0.97):\n",
        "    filtered_sig = torch.empty_like(waveform)\n",
        "    filtered_sig[1:] = waveform[1:] - coeff*waveform[:-1]\n",
        "    filtered_sig[0] = waveform[0]\n",
        "    return filtered_sig\n",
        "\n",
        "\n",
        "def add_time_stretch(audio, fs, stretch_rate):\n",
        "    \"\"\"\n",
        "    Adds time stretch to <clean> audio by <stretch_rate> factor.\n",
        "    Parameters:\n",
        "        audio (tensor): waveform, dims: (N,)\n",
        "        fs (float): audio sample rate\n",
        "        stretch_rate (float): playback rate\n",
        "    Returns:\n",
        "        audio_stretch (tensor): time stretched waveform dims: (N*<stretch_rate>,)\n",
        "\n",
        "    \"\"\"\n",
        "    audio_stretch = time_stretch(audio.unsqueeze(0).unsqueeze(0), 1/stretch_rate, fs)\n",
        "    # assert len(audio)/stretch_rate == len(audio_stretch), f\"stretched audio length mismatch. Expected {len(audio)*stretch_rate}, got {len(audio_stretch)}\"\n",
        "    return audio_stretch.squeeze_()\n",
        "\n",
        "\n",
        "def add_pitch_shift_rb(y, sr, shift, tmpdir=None):\n",
        "    \"\"\"\n",
        "    Adds pitch shift to <y> audio sampled at <sr> by <shift> semitones. It calls rubberband package directly; does not use pyrubberband package.\n",
        "    \"\"\"\n",
        "    if isinstance(y, np.ndarray) is False:\n",
        "        y = y.numpy()\n",
        "\n",
        "    if tmpdir is not None:\n",
        "        tempfile.tempdir = tmpdir\n",
        "\n",
        "    # Get the input and output tempfile\n",
        "    fd, infile = tempfile.mkstemp(suffix='.wav')\n",
        "    os.close(fd)\n",
        "    fd, outfile = tempfile.mkstemp(suffix='.wav')\n",
        "    os.close(fd)\n",
        "\n",
        "    sf.write(infile, y, sr)\n",
        "    command = [\"rubberband\", \"-q\", \"--pitch\", str(shift), infile, outfile]\n",
        "    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    y_out, _ = sf.read(outfile, always_2d=True, dtype=y.dtype)\n",
        "    if y.ndim == 1:\n",
        "        y_out = np.squeeze(y_out)\n",
        "\n",
        "    os.unlink(infile)\n",
        "    os.unlink(outfile)\n",
        "    return torch.from_numpy(y_out).type(torch.float32)\n",
        "\n",
        "\n",
        "def add_pitch_shift(audio, fs, semitone_shift, use_rb=True):\n",
        "    \"\"\"\n",
        "    Adds pitch shift to <clean> audio by <semitone_shit> semitones.\n",
        "    Parameters:\n",
        "        audio (tensor): clean waveform, dims: (N,)\n",
        "        fs (float): audio sample rate\n",
        "        semitone_shift (float): semitones (can be between -12 and 12, but keep it low for good sound quality)\n",
        "    Returns:\n",
        "        audio_shift: pitch shifted added signal (tensor), dims: (N,)\n",
        "\n",
        "    \"\"\"\n",
        "    if use_rb:\n",
        "        audio_shift = pyrb.pitch_shift(audio.numpy(), fs, semitone_shift)\n",
        "        return torch.from_numpy(audio_shift).type(torch.float32)\n",
        "    else:\n",
        "        audio_shift = torchaudio.functional.pitch_shift(audio, fs, semitone_shift)\n",
        "        return audio_shift\n",
        "\n",
        "def add_noise(audio, noise, snr):\n",
        "    \"\"\"\n",
        "    Adds background <noise> to <clean> signal at desired <SNR> level\n",
        "    Parameters:\n",
        "        audio (tensor): clean waveform, dims: (N,)\n",
        "        noise (tensor): noise waveform, dims: (M,)\n",
        "        snr (int): SNR level in dB\n",
        "    Returns:\n",
        "        noisy_audio: noisy signal (tensor), dims: (N,)\n",
        "    \"\"\"\n",
        "    # make equal lengths for clean and noise signals\n",
        "    if len(audio) >= len(noise):\n",
        "        reps = torch.ceil(torch.tensor(len(audio)/len(noise))).int()\n",
        "        noise = torch.tile(noise, (reps,))[:len(audio)]\n",
        "    else:\n",
        "        start_idx = torch.randint(len(noise) - len(audio), (1,))\n",
        "        noise = noise[start_idx:start_idx+len(audio)]\n",
        "\n",
        "    assert len(noise) == len(audio), f\"noise signal {len(noise)} and clean signal {len(audio)} length mismatch\"\n",
        "\n",
        "    # add noise at desired snr\n",
        "    audio_rms = torch.mean(torch.pow(audio, 2))\n",
        "    noise_rms = torch.mean(torch.pow(noise, 2))\n",
        "    factor = torch.pow((audio_rms/noise_rms)/torch.pow(torch.tensor(10), (snr/10)), 0.5)\n",
        "    noise = factor*noise\n",
        "    noisy_audio = audio + noise\n",
        "    assert 10*torch.log10(audio_rms/torch.mean(torch.pow(noise, 2))) - snr < 1e-4, f\"snr mismatch {10*torch.log10(audio_rms/torch.mean(torch.pow(noise, 2))), snr, len(audio), len(noise), audio_rms, torch.mean(torch.pow(noise, 2)), noise_rms, factor, audio, torch.count_nonzero(audio)}\"\n",
        "    return noisy_audio\n",
        "\n",
        "\n",
        "def add_reverb(clean, rir):\n",
        "    \"\"\"\n",
        "    Filters <clean> signal with <rir> to get reverberation effect\n",
        "    Parameters:\n",
        "        clean (tensor): clean waveform, dims: (N,)\n",
        "        rir (tensor): room impulse response, dims: (M,)\n",
        "    Returns:\n",
        "        reverb added signal (tensor), dims: (N,)\n",
        "    \"\"\"\n",
        "    clean = clean.numpy()\n",
        "    rir = rir.numpy()\n",
        "    rir = rir/np.linalg.norm(rir)\n",
        "    # filering\n",
        "    p_max = np.argmax(np.abs(rir))\n",
        "    filtered_clean = signal.convolve(clean, rir, mode=\"full\")\n",
        "\n",
        "    # time offset\n",
        "    e = np.empty_like(filtered_clean, dtype=np.float32)\n",
        "    e[-p_max:] = 0.0\n",
        "    e[:-p_max] = filtered_clean[p_max:]\n",
        "    # filtered_clean = e.copy()\n",
        "    # e=None\n",
        "    filtered_clean = e[:len(clean)]\n",
        "    assert(len(filtered_clean)==len(clean))\n",
        "    filtered_clean = torch.from_numpy(filtered_clean)\n",
        "    return filtered_clean\n",
        "\n",
        "\n",
        "def add_noise_reverb(audio, noise, snr, rir):\n",
        "    \"\"\"\n",
        "    Adds background <noise> at desired <snr> level and reveberation using <rir> to <clean> signal\n",
        "    Parameters:\n",
        "        audio (tensor): clean waveform, dims: (N,)\n",
        "        noise (tensor): noise waveform, dims: (M,)\n",
        "        snr (int): SNR level in dB\n",
        "        rir (tensor): room impulse response, dims: (M,)\n",
        "    Returns:\n",
        "        noise and reverb added signal (tensor), dims: (N,)\n",
        "    \"\"\"\n",
        "    audio_reverb = add_reverb(audio, rir)\n",
        "    noise_reverb = add_reverb(noise, rir)\n",
        "    noise_reverb_clean = add_noise(audio_reverb, noise_reverb, snr)\n",
        "    return noise_reverb_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-vX6VK0D7g3",
        "outputId": "6c75bbae-2b44-4d35-95ac-d4e07493adea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 331M/331M [00:15<00:00, 22.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "librispeech_test = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-qER2e8Ex6s",
        "outputId": "fd7069fd-1b3a-4c47-e78c-745a06eec3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.2)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.10.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq) (12.4.99)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291782 sha256=1f411037e3a85c86947e6a395b54c883bbeffac98b03be07b8c2195602872ebd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=1cf0e641748fcf9a748e0d2f85b39b91725cc39d4f230c388ffe320d2a136e8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "# Read paths of all flac files in the directory\n",
        "flac_files = glob.glob(\"/content/LibriSpeech/test-clean/**/*.flac\", recursive=True)\n",
        "\n",
        "# Select the first 1000 files\n",
        "files_subset = flac_files[:10]\n",
        "\n",
        "print(files_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab-Lhtho62r8",
        "outputId": "fc1e72be-26b8-41ed-ed44-575570b9da3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/LibriSpeech/test-clean/1580/141084/1580-141084-0033.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0020.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0042.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0048.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0049.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0021.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0024.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0028.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0034.flac', '/content/LibriSpeech/test-clean/1580/141084/1580-141084-0013.flac']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3LwkBpk1aH2",
        "outputId": "740a6e38-41b4-42cf-bbef-b5941b210327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "cagOqgmUD_PU",
        "outputId": "c0dbc05d-7326-4185-e127-5cb8e35f6bed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "unexpected EOF, expected 128355833 more bytes. The file might be corrupted.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4128a828098c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/vq-wav2vec.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_ensemble_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         typed_storage._untyped_storage._set_from_file(\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m             torch._utils._element_size(typed_storage.dtype))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 128355833 more bytes. The file might be corrupted."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import fairseq\n",
        "\n",
        "cp = torch.load('/content/vq-wav2vec.pt')\n",
        "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp])\n",
        "model = model[0]\n",
        "model.eval()\n",
        "\n",
        "for filepath in files_subset:\n",
        "    waveform = read_audio(filepath)\n",
        "    if waveform is not None:\n",
        "        # Process the waveform\n",
        "        wav_input_16khz = waveform.unsqueeze(0)  # Add batch dimension\n",
        "        z = model.feature_extractor(wav_input_16khz)\n",
        "        _, idxs = model.vector_quantizer.forward_idx(z)\n",
        "        print(f\"Processed {filepath}. Shape: {idxs.shape}\")\n",
        "    else:\n",
        "        print(f\"Failed to process: {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import fairseq\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "def read_audio(filepath, fs=16000, mono=True, normalize=False, preemphasis=False):\n",
        "    \"\"\"\n",
        "    Reads audio file stored at <filepath>\n",
        "    Parameters:\n",
        "        filepath (str): audio file path\n",
        "        fs (int, optional): samping rate\n",
        "        mono (boolean, optional): return single channel\n",
        "        normalize(boolean, optional): peak normalization of signal\n",
        "        preemphasis (boolean, optional): apply pre-emphasis filter\n",
        "    Returns:\n",
        "        waveform (tensor): audio signal, dim(N,)\n",
        "    \"\"\"\n",
        "    assert isinstance(filepath, str), \"filepath must be specified as string\"\n",
        "    assert os.path.exists(filepath), f\"{filepath} does not exist.\"\n",
        "\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(filepath)\n",
        "        # mono channel\n",
        "        if waveform.shape[0] == 2 and mono is True:\n",
        "            waveform = waveform[0]\n",
        "        else:\n",
        "            waveform = waveform.reshape(-1)\n",
        "        # preemphasis\n",
        "        if preemphasis:\n",
        "            waveform = pre_emphasis(waveform)\n",
        "        # resample\n",
        "        if sr != fs:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=fs, dtype=waveform.dtype)\n",
        "            waveform = resampler(waveform)\n",
        "        # normalize\n",
        "        if normalize:\n",
        "            waveform = rms_normalize(waveform)\n",
        "        return waveform\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Download the dataset from the provided link\n",
        "dataset_url = \"http://www.openslr.org/resources/12\"\n",
        "# Replace \"/path/to/save/dataset\" with the directory where you want to save the dataset\n",
        "root_dir = \"/content/sample_data/dataset\"  # Replace with the directory where you want to save the dataset\n",
        "torchaudio.datasets.LIBRISPEECH(root=root_dir, download=True)\n",
        "\n",
        "# Download the pre-trained model checkpoint file from the provided link\n",
        "model_checkpoint_url = \"https://dl.fbaipublicfiles.com/fairseq/wav2vec/vq-wav2vec.pt\"\n",
        "model_checkpoint_path = \"/path/to/save/model_checkpoint.pt\"  # Replace with the path where you want to save the model checkpoint\n",
        "torch.hub.download_url_to_file(model_checkpoint_url, model_checkpoint_path)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_checkpoint_path])\n",
        "model = model[0]\n",
        "model.eval()\n",
        "\n",
        "# Load audio file from the dataset\n",
        "# Replace '/path/to/audio_file.wav' with the path to the audio file from the downloaded dataset\n",
        "audio_filepath = os.path.join(root_dir, \"path\", \"to\", \"audio_file.wav\")  # Replace with the path to your audio file\n",
        "waveform = read_audio(audio_filepath)\n",
        "\n",
        "# Ensure the waveform is loaded successfully\n",
        "if waveform is not None:\n",
        "    # Convert to tensor and normalize\n",
        "    wav_input_16khz = waveform.unsqueeze(0).float() / 32768.0\n",
        "\n",
        "    # Extract features and quantize\n",
        "    z = model.feature_extractor(wav_input_16khz)\n",
        "    _, idxs = model.vector_quantizer.forward_idx(z)\n",
        "\n",
        "    # Print the indices\n",
        "    print(idxs)\n",
        "else:\n",
        "    print(\"Failed to load audio.\")\n"
      ],
      "metadata": {
        "id": "JgrN4_bK9H1D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "f326d4f4-83b8-4613-b814-9f90a19691cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [03:57<00:00, 26.9MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/save/model_checkpoint.pt.3b978c58d7b149e29d5085411af92061.partial'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7ec764c114a7>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel_checkpoint_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://dl.fbaipublicfiles.com/fairseq/wav2vec/vq-wav2vec.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/save/model_checkpoint.pt\"\u001b[0m  \u001b[0;31m# Replace with the path where you want to save the model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Load the pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mtmp_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.partial'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/save/model_checkpoint.pt.3b978c58d7b149e29d5085411af92061.partial'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RT_zimMYayF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}